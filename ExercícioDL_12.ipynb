{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NevesJulio/Exercicios_DL/blob/main/Exerc%C3%ADcioDL_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "541479dd",
      "metadata": {
        "id": "541479dd"
      },
      "source": [
        "# Redes Neurais Convolucionais\n",
        "\n",
        "Neste notebook, investigaremos as Redes Neurais Convolucionais (Convolutional Neural Networks, ou CNNs), uma classe de arquiteturas de redes neurais que se tornou o padrão para tarefas de visão computacional. Diferentemente de um Perceptron Multicamadas (MLP), que trata a entrada como um vetor unidimensional, as CNNs são projetadas para processar dados que possuem uma topologia de grade, como imagens.\n",
        "\n",
        "Para demonstrar sua eficácia, construiremos e treinaremos dois modelos para a tarefa de classificação de dígitos manuscritos do dataset MNIST:\n",
        "\n",
        "1.  Um **modelo linear (MLP)**, que servirá como nossa baseline.\n",
        "2.  Um **modelo convolucional (CNN)**, que emprega camadas convolucionais para aprender hierarquias de features espaciais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3046386",
      "metadata": {
        "id": "b3046386"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor, Normalize, Compose\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a846ac3b",
      "metadata": {
        "id": "a846ac3b"
      },
      "source": [
        "## Dataset MNIST\n",
        "\n",
        "Iniciamos preparando o dataset MNIST. Utilizaremos subconjuntos de 5.000 imagens para treino e 1.000 para validação. As imagens serão convertidas para tensores e normalizadas (média `0.1307`, desvio padrão `0.3081`) para otimizar a convergência do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b13efa5e",
      "metadata": {
        "id": "b13efa5e"
      },
      "outputs": [],
      "source": [
        "transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "full_train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "full_val_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_indices = torch.arange(5000)\n",
        "val_indices = torch.arange(1000)\n",
        "\n",
        "train_subset = Subset(full_train_dataset, train_indices)\n",
        "val_subset = Subset(full_val_dataset, val_indices)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Tamanho do subset de treino: {len(train_subset)} imagens\")\n",
        "print(f\"Tamanho do subset de validação: {len(val_subset)} imagens\")\n",
        "print(f\"Número de batches no train_loader: {len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1184fd09",
      "metadata": {
        "id": "1184fd09"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def evaluate_model(model, val_loader, criterion):\n",
        "    model.eval()  # Coloca o modelo em modo de avaliação\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_val_loss, accuracy\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_accuracy': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Modo de treinamento\n",
        "        running_train_loss = 0.0\n",
        "\n",
        "        # Loop de treinamento\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_train_loss / len(train_loader)\n",
        "\n",
        "        # Avaliação ao final da época\n",
        "        avg_val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n",
        "\n",
        "        # Armazena as métricas\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_accuracy'].append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, \"\n",
        "              f\"Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"Treinamento concluído.\")\n",
        "    return history\n",
        "\n",
        "def plot_curves(history):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot de Perda (Loss)\n",
        "    ax1.plot(history['train_loss'], label='Perda de Treinamento')\n",
        "    ax1.plot(history['val_loss'], label='Perda de Validação')\n",
        "    ax1.set_title('Curvas de Perda')\n",
        "    ax1.set_xlabel('Épocas')\n",
        "    ax1.set_ylabel('Perda')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot de Acurácia\n",
        "    ax2.plot(history['val_accuracy'], label='Acurácia de Validação', color='g')\n",
        "    ax2.set_title('Curva de Acurácia')\n",
        "    ax2.set_xlabel('Épocas')\n",
        "    ax2.set_ylabel('Acurácia (%)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32970e15",
      "metadata": {
        "id": "32970e15"
      },
      "source": [
        "## Baseline: Modelo Linear (MLP)\n",
        "\n",
        "Nossa primeira abordagem será um Perceptron Multicamadas (MLP) simples. A imagem de entrada `[1, 28, 28]` é \"achatada\" (flatten) para um vetor de 784 elementos, perdendo sua estrutura espacial 2D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cf23d61",
      "metadata": {
        "id": "8cf23d61"
      },
      "outputs": [],
      "source": [
        "mlp_model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(28 * 28, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ")\n",
        "\n",
        "print(f\"Modelo MLP - Número de parâmetros: {count_parameters(mlp_model):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b021e984",
      "metadata": {
        "id": "b021e984"
      },
      "outputs": [],
      "source": [
        "# Definindo o critério de perda e o otimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
        "\n",
        "# Treinando o modelo e coletando o histórico\n",
        "mlp_history = train_model(mlp_model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n",
        "\n",
        "# Plotando as curvas de aprendizado\n",
        "plot_curves(mlp_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad5b42b",
      "metadata": {
        "id": "3ad5b42b"
      },
      "source": [
        "## Modelo Convolucional (CNN)\n",
        "\n",
        "Agora, implementaremos a CNN. Sua arquitetura utiliza camadas especializadas (`Conv2d`, `MaxPool2d`) para preservar e explorar a estrutura espacial das imagens, baseando-se nos princípios de **localidade de conexões** e **compartilhamento de parâmetros**.\n",
        "\n",
        "* A Camada Convolucional (`nn.Conv2d`): Aplica um conjunto de filtros (kernels) à imagem para gerar mapas de features, detectando padrões como bordas e texturas.\n",
        "* A Camada de Pooling (`nn.MaxPool2d`): Reduz as dimensões espaciais dos mapas de features, diminuindo a carga computacional e criando uma representação mais robusta a pequenas translações."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6102ea62",
      "metadata": {
        "id": "6102ea62"
      },
      "outputs": [],
      "source": [
        "cnn_model = nn.Sequential(\n",
        "    # A entrada tem shape [N, 1, 28, 28]\n",
        "    nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
        "    # Shape: [N, 16, 28, 28]\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    # Shape: [N, 16, 14, 14]\n",
        "\n",
        "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "    # Shape: [N, 32, 14, 14]\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    # Shape: [N, 32, 7, 7]\n",
        "\n",
        "    nn.Flatten(),\n",
        "    # Shape: [N, 32*7*7] = [N, 1568]\n",
        "    nn.Linear(32 * 7 * 7, 10)\n",
        "    # Shape final: [N, 10]\n",
        ")\n",
        "\n",
        "# Contagem de parâmetros\n",
        "print(f\"Modelo CNN - Número de parâmetros: {count_parameters(cnn_model):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15e24992",
      "metadata": {
        "id": "15e24992"
      },
      "outputs": [],
      "source": [
        "# Definindo o critério de perda e o otimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Treinando o modelo e coletando o histórico\n",
        "cnn_history = train_model(cnn_model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n",
        "\n",
        "# Plotando as curvas de aprendizado\n",
        "plot_curves(cnn_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27eabc74",
      "metadata": {
        "id": "27eabc74"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1e00d74",
      "metadata": {
        "id": "a1e00d74"
      },
      "source": [
        "### Exercício 1\n",
        "\n",
        "Em vez de usar `nn.Sequential` para todo o modelo, uma prática mais comum e flexível é definir a arquitetura dentro de uma classe que herda de `nn.Module`. Isso nos permite organizar o modelo em seções lógicas.\n",
        "\n",
        "Neste exercício, construa um modelo mais profundo, com três ou mais blocos convolucionais, e os organize em duas partes dentro de uma classe:\n",
        "1.  Um extrator de features (`feature_extractor`), contendo as camadas convolucionais e de pooling.\n",
        "2.  Um classificador (`classifier`), contendo as camadas `Flatten`, `Linear` e `Dropout`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19f4f00c",
      "metadata": {
        "id": "19f4f00c"
      },
      "source": [
        "### Exercício 2\n",
        "\n",
        "Considere que ao invés do MNIST, os dados de entrada são imagens RGB no formato `(3, 32, 32)`. Implemente uma CNN para classificar essas imagens em 10 classes."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}