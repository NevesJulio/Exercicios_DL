{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NevesJulio/Exercicios_DL/blob/main/Atividade_DeepLearning_Labubu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf78b4e9",
      "metadata": {
        "id": "cf78b4e9"
      },
      "source": [
        "# Atividade: CNNs para Classificação\n",
        "\n",
        "Neste notebook, iremos preparar nosso próprio dataset e treinar um modelo de classificação de imagens."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "108587c0",
      "metadata": {
        "id": "108587c0"
      },
      "source": [
        "## Preparando os dados\n",
        "\n",
        "Os dados desta atividade serão baixados da internet. Utilizaremos para isso buscadores comuns. Em seguida, dividiremos em treinamento e validação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9702c4cc",
        "outputId": "fcd69783-e90f-4ed0-a66a-63a1676ad8d7"
      },
      "source": [
        "%pip install icrawler"
      ],
      "id": "9702c4cc",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting icrawler\n",
            "  Downloading icrawler-0.6.10-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from icrawler) (4.13.5)\n",
            "Collecting bs4 (from icrawler)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from icrawler) (5.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from icrawler) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from icrawler) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from icrawler) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from icrawler) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->icrawler) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->icrawler) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (2025.10.5)\n",
            "Downloading icrawler-0.6.10-py3-none-any.whl (36 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4, icrawler\n",
            "Successfully installed bs4-0.0.2 icrawler-0.6.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7b74ce50",
      "metadata": {
        "id": "7b74ce50"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from icrawler.builtin import GoogleImageCrawler, BingImageCrawler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b98d4ebb",
      "metadata": {
        "id": "b98d4ebb"
      },
      "source": [
        "### Adquirindo as Imagens\n",
        "\n",
        "Utilizaremos o iCrawler para baixar imagens em buscadores através de termos especificados. Defina sua lista de classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bb2c5e01",
      "metadata": {
        "id": "bb2c5e01"
      },
      "outputs": [],
      "source": [
        "def download_images(keyword, folder, n_total=100):\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    downloaded = len(os.listdir(folder))\n",
        "    remaining = n_total - downloaded\n",
        "\n",
        "    while downloaded < n_total:\n",
        "        crawler = GoogleImageCrawler(storage={'root_dir': folder})\n",
        "        crawler.crawl(keyword=keyword, max_num=remaining, file_idx_offset=downloaded)\n",
        "        downloaded = len(os.listdir(folder))\n",
        "        remaining = n_total - downloaded\n",
        "        print(f\"Downloaded {downloaded}/{n_total}\")\n",
        "\n",
        "    print(\"Download complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4008f42c",
      "metadata": {
        "id": "4008f42c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a65c0823-9ce2-494b-8449-6213010b67fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Exception caught when downloading file https://cdn-images.farfetch-contents.com/31/78/64/07/31786407_61234927_600.jpg, error: HTTPSConnectionPool(host='cdn-images.farfetch-contents.com', port=443): Read timed out. (read timeout=5), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file https://cdn-images.farfetch-contents.com/31/78/64/07/31786407_61234927_600.jpg, error: HTTPSConnectionPool(host='cdn-images.farfetch-contents.com', port=443): Read timed out. (read timeout=5), remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file https://cdn-images.farfetch-contents.com/31/78/64/07/31786407_61234927_600.jpg, error: HTTPSConnectionPool(host='cdn-images.farfetch-contents.com', port=443): Read timed out. (read timeout=5), remaining retry times: 0\n",
            "ERROR:downloader:Response status code 401, file https://i.guim.co.uk/img/media/2a75b01d900399e8c42de70ec72c05e4787b7092/301_0_2501_2001/master/2501.jpg\n",
            "ERROR:downloader:Response status code 403, file https://i.redd.it/labubu-quality-help-advice-please-v0-6xth6ygpxh9e1.jpg\n",
            "ERROR:downloader:Response status code 403, file https://media.asiaone.com/sites/default/files/styles/article_top_image/public/original_images/Sep2024/20242709%20Labubu.jpg\n",
            "ERROR:downloader:Response status code 406, file https://www.usatoday.com/gcdn/authoring/authoring-images/2025/07/23/USAT/85345394007-getty-images-2220163705.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 63/100\n",
            "Downloaded 64/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Exception caught when downloading file https://cdn-images.farfetch-contents.com/31/78/64/07/31786407_61234927_600.jpg, error: HTTPSConnectionPool(host='cdn-images.farfetch-contents.com', port=443): Read timed out. (read timeout=5), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file https://cdn-images.farfetch-contents.com/31/78/64/07/31786407_61234927_600.jpg, error: HTTPSConnectionPool(host='cdn-images.farfetch-contents.com', port=443): Read timed out. (read timeout=5), remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file https://cdn-images.farfetch-contents.com/31/78/64/07/31786407_61234927_600.jpg, error: HTTPSConnectionPool(host='cdn-images.farfetch-contents.com', port=443): Read timed out. (read timeout=5), remaining retry times: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 100/100\n",
            "Download complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://archives.bulbagarden.net/media/upload/thumb/4/49/Ash_Pikachu.png\n",
            "ERROR:downloader:Response status code 403, file https://external-preview.redd.it/AE6Jw9dqaSdzrz_ZWl_IrwwSd5ViFyiA_sKMELQ9YPs.png\n",
            "ERROR:downloader:Exception caught when downloading file https://www.png, error: HTTPSConnectionPool(host='www.png', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7a089021df10>: Failed to resolve 'www.png' ([Errno -2] Name or service not known)\")), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file https://www.png, error: HTTPSConnectionPool(host='www.png', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7a087fc252b0>: Failed to resolve 'www.png' ([Errno -2] Name or service not known)\")), remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file https://www.png, error: HTTPSConnectionPool(host='www.png', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7a087fc2faa0>: Failed to resolve 'www.png' ([Errno -2] Name or service not known)\")), remaining retry times: 0\n",
            "ERROR:downloader:Response status code 403, file https://archives.bulbagarden.net/media/upload/thumb/b/b2/Roy_Captain_Pikachu.png\n",
            "ERROR:downloader:Response status code 403, file https://cdn12.picryl.com/photo/2016/12/31/pokemon-pikachu-game-e24b77-1024.jpg\n",
            "ERROR:downloader:Response status code 403, file https://cdn1.tedsby.com/storage/1/3/3/1330542/stuffed-fantasy-creature-pikachu.jpg\n",
            "ERROR:downloader:Response status code 403, file https://img.asmedia.epimg.net/resizer/v2/WCA7OQPAQNDXBK5WCSY6L5HCOY.jpg\n",
            "ERROR:downloader:Response status code 404, file https://www.muraldecal.com/en/img/pkm012-png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 68/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://archives.bulbagarden.net/media/upload/thumb/4/49/Ash_Pikachu.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 100/100\n",
            "Download complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://www.petz.com.br/blog/wp-content/uploads/2025/04/macaco-sagui-4-scaled.jpg\n",
            "ERROR:downloader:Response status code 404, file https://t3.ftcdn.net/jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.istockphoto.com/id/1472424290/photo/sagui.jpg\n",
            "ERROR:downloader:Response status code 429, file https://live.staticflickr.com/1560/26191448156_b3d285ddfd_b.jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.gettyimages.com/id/1367935230/photo/marmosets-sagui-monkey.jpg\n",
            "ERROR:downloader:Response status code 429, file https://live.staticflickr.com/1918/44299593494_0f88a0918e_b.jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.gettyimages.com/id/975257608/photo/sagui.jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.istockphoto.com/id/164943444/photo/sagui-dwarf-monkey.jpg\n",
            "ERROR:downloader:Response status code 429, file https://live.staticflickr.com/4750/26475814308_cf95905908_b.jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.gettyimages.com/id/999647676/photo/sagui.jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.istockphoto.com/id/1442980403/photo/sagui-de-cara-branca-white-headed-marmoset.jpg\n",
            "ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Sagui_-_Callitrichidae.jpg\n",
            "ERROR:downloader:Response status code 429, file https://live.staticflickr.com/7402/9273979455_6c254e9627_b.jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.istockphoto.com/id/825546270/photo/white-tuft-sagui-in-forest.jpg\n",
            "ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Saguinus_tripartitus_-_Golden-mantled_Tamarin.jpg\n",
            "ERROR:downloader:Response status code 404, file https://as1.ftcdn.net/jpg\n",
            "ERROR:downloader:Response status code 403, file https://static.vecteezy.com/system/resources/previews/040/970/980/large_2x/sagui-monkey-in-the-wild-eating-a-piece-of-banana-in-the-countryside-of-sao-paulo-brazil-photo.jpg\n",
            "ERROR:downloader:Response status code 403, file https://preview.redd.it/i-finded-a-cute-monkey-in-brazil-called-sagui-v0-v2pafbspulad1.png\n",
            "ERROR:downloader:Response status code 403, file https://s3.amazonaws.com/media.jungledragon.com/images/5920/120450_small.jpg\n",
            "ERROR:downloader:Response status code 403, file https://static.vecteezy.com/system/resources/previews/040/970/978/large_2x/sagui-monkey-in-the-wild-eating-a-piece-of-banana-in-the-countryside-of-sao-paulo-brazil-photo.jpg\n",
            "ERROR:downloader:Response status code 403, file https://preview.redd.it/a-brasilian-sagui-monkey-v0-62xi7fiyeyge1.jpg\n",
            "ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Saguinus_midas_flk24863753.jpg\n",
            "ERROR:downloader:Response status code 403, file https://protecaoanimal.curitiba.pr.gov.br/images/animais-silvestres/vertebrados/mamiferos/72.sagui-Morato-n-61.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 52/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://www.petz.com.br/blog/wp-content/uploads/2025/04/macaco-sagui-4-scaled.jpg\n",
            "ERROR:downloader:Response status code 404, file https://t3.ftcdn.net/jpg\n",
            "ERROR:downloader:Response status code 429, file https://live.staticflickr.com/1560/26191448156_b3d285ddfd_b.jpg\n",
            "ERROR:downloader:Response status code 403, file https://mid-educacao.curitiba.pr.gov.br/2022/8/jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.gettyimages.com/id/1367935230/photo/marmosets-sagui-monkey.jpg\n",
            "ERROR:downloader:Response status code 429, file https://live.staticflickr.com/1918/44299593494_0f88a0918e_b.jpg\n",
            "ERROR:downloader:Response status code 400, file https://www.ucs.br/zoo/_next/image?url=https%3A%2F%2Fsou.ucs.br%2Fapi%2Fzoo%2Fuploads%2Fimagens%2Fsagui-de-tufo-branco.png\n",
            "ERROR:downloader:Response status code 400, file https://media.gettyimages.com/id/975257608/photo/sagui.jpg\n",
            "ERROR:downloader:Response status code 429, file https://live.staticflickr.com/4750/26475814308_cf95905908_b.jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.gettyimages.com/id/999647676/photo/sagui.jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.istockphoto.com/id/1442980403/photo/sagui-de-cara-branca-white-headed-marmoset.jpg\n",
            "ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Sagui_-_Callitrichidae.jpg\n",
            "ERROR:downloader:Response status code 404, file https://as2.ftcdn.net/jpg\n",
            "ERROR:downloader:Response status code 429, file https://live.staticflickr.com/7402/9273979455_6c254e9627_b.jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.istockphoto.com/id/825546270/photo/white-tuft-sagui-in-forest.jpg\n",
            "ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Saguinus_tripartitus_-_Golden-mantled_Tamarin.jpg\n",
            "ERROR:downloader:Response status code 404, file https://as1.ftcdn.net/jpg\n",
            "ERROR:downloader:Response status code 403, file https://static.vecteezy.com/system/resources/previews/040/970/980/large_2x/sagui-monkey-in-the-wild-eating-a-piece-of-banana-in-the-countryside-of-sao-paulo-brazil-photo.jpg\n",
            "ERROR:downloader:Response status code 403, file https://preview.redd.it/i-finded-a-cute-monkey-in-brazil-called-sagui-v0-v2pafbspulad1.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 100/100\n",
            "Download complete!\n"
          ]
        }
      ],
      "source": [
        "search_terms = {\n",
        "     \"Labubu\": \"Labubu\", # nome da classe: termo que será usado na busca\n",
        "     \"Pikachu\": \"pikachu\",\n",
        "     \"sagui\": \"sagui\"\n",
        "\n",
        "}\n",
        "\n",
        "for label, term in search_terms.items():\n",
        "    download_images(term, f\"data/Julio/{label}\", n_total=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "from PIL import Image\n",
        "import glob"
      ],
      "metadata": {
        "id": "_gsCiDb7WYWk"
      },
      "id": "_gsCiDb7WYWk",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms, models\n"
      ],
      "metadata": {
        "id": "Vp4CXD1td043"
      },
      "id": "Vp4CXD1td043",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0d931e35",
      "metadata": {
        "id": "0d931e35"
      },
      "source": [
        "### Treinamento e Validação\n",
        "\n",
        "Dividiremos as imagens baixadas nas pastas `train` e `val`. Defina uma porcentagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "88181559",
      "metadata": {
        "id": "88181559"
      },
      "outputs": [],
      "source": [
        "def split_train_val(root_dir, train_ratio=0.8, seed=42):\n",
        "    random.seed(seed)\n",
        "\n",
        "    train_dir = root_dir + \"_split/train\"\n",
        "    val_dir = root_dir + \"_split/val\"\n",
        "\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "    for class_name in os.listdir(root_dir):\n",
        "        class_path = os.path.join(root_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        images = [os.path.join(class_path, f) for f in os.listdir(class_path)]\n",
        "        images = [f for f in images if os.path.isfile(f)]\n",
        "        random.shuffle(images)\n",
        "\n",
        "        n_train = int(len(images) * train_ratio)\n",
        "\n",
        "        train_class_dir = os.path.join(train_dir, class_name)\n",
        "        val_class_dir = os.path.join(val_dir, class_name)\n",
        "        os.makedirs(train_class_dir, exist_ok=True)\n",
        "        os.makedirs(val_class_dir, exist_ok=True)\n",
        "\n",
        "        for img in images[:n_train]:\n",
        "            shutil.copy(img, os.path.join(train_class_dir, os.path.basename(img)))\n",
        "        for img in images[n_train:]:\n",
        "            shutil.copy(img, os.path.join(val_class_dir, os.path.basename(img)))\n",
        "\n",
        "        print(f\"{class_name}: {n_train} train, {len(images)-n_train} val\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_train_val(\"data/Julio\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6tavrk4XjRA",
        "outputId": "3aa2f00f-836f-4db5-9caf-b2fc8a9d6c74"
      },
      "id": "F6tavrk4XjRA",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sagui: 80 train, 20 val\n",
            "Pikachu: 80 train, 20 val\n",
            "Labubu: 80 train, 20 val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9974e910",
      "metadata": {
        "id": "9974e910"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Implemente um Dataset PyTorch que carregue as imagens baixadas com suas respectivas classes. Aplique data augmentation e carregue em batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f91b901e",
      "metadata": {
        "id": "f91b901e"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.class_names = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
        "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.class_names)}\n",
        "\n",
        "        self.data = []\n",
        "        for class_name in self.class_names:\n",
        "            class_dir = os.path.join(self.root_dir, class_name)\n",
        "            for filename in os.listdir(class_dir):\n",
        "                if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    path = os.path.join(class_dir, filename)\n",
        "                    item = (path, self.class_to_idx[class_name])\n",
        "                    self.data.append(item)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.data[idx]\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"Data/DataSet\"\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LR = 1e-4\n",
        "DEVICE = \"cuda\"\n",
        "MODEL_NAME = \"resnet50\"\n",
        "n_runs = 1\n",
        "histories = {}\n",
        "num_classes = 2\n",
        "DROPOUT_RATE = 0.4\n",
        "\n",
        "\n",
        "train_dataset = CustomImageDataset(\"data/Julio_split/train\")\n",
        "val_dataset   = CustomImageDataset(\"data/Julio_split/val\")\n"
      ],
      "metadata": {
        "id": "9NRvmmHUbLeo"
      },
      "id": "9NRvmmHUbLeo",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1, pin_memory=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1, pin_memory=True)"
      ],
      "metadata": {
        "id": "lO4vnCaeZZmj"
      },
      "id": "lO4vnCaeZZmj",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Obtém um batch de dados de treino\n",
        "#inputs, classes = next(iter(train_dataloader))\n",
        "\n",
        "# # Cria uma grade a partir do batch\n",
        "# out = utils.make_grid(inputs)\n",
        "\n",
        "# imshow(out, title=[class_names[x] for x in classes])"
      ],
      "metadata": {
        "id": "w438DBhUXW6T"
      },
      "id": "w438DBhUXW6T",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "83ab6c9e",
      "metadata": {
        "id": "83ab6c9e"
      },
      "source": [
        "## Definição do Modelo\n",
        "\n",
        "Defina aqui o modelo que será utilizado, sendo implementação própria ou um modelo pré-treinado."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomHead(nn.Module):\n",
        "    def __init__(self, in_features, num_classes, dropout_p=0.5):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.BatchNorm1d(in_features),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Linear(in_features, 512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "zfspHSbFdQR8"
      },
      "id": "zfspHSbFdQR8",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "model.fc = CustomHead(model.fc.in_features, 3, DROPOUT_RATE)\n",
        "model = model.to(DEVICE)"
      ],
      "metadata": {
        "id": "YuXtH_oVeMgb"
      },
      "id": "YuXtH_oVeMgb",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f7634074",
      "metadata": {
        "id": "f7634074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "d18418aa-61b0-4622-9aba-8e1f22bd276e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'CustomHead' object has no attribute 'in_features'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-713114057.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomHead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROPOUT_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1960\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1962\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1963\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CustomHead' object has no attribute 'in_features'"
          ]
        }
      ],
      "source": [
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if \"layer3\" not in name and \"layer4\" not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "model.fc = CustomHead(model.fc.in_features, num_classes, DROPOUT_RATE)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "filter(lambda p: p.requires_grad, model.parameters()),\n",
        "lr=1e-4,\n",
        "weight_decay=0.02  # L2 regularization\n",
        ")\n",
        "\n",
        "\n",
        "for i in range(n_runs):\n",
        "    print(f\"\\n===== Run {i+1}/{n_runs} =====\")\n",
        "\n",
        "    scheduler = OneCycleLR(optimizer, max_lr=LR, steps_per_epoch=len(train_loader), epochs=EPOCHS)\n",
        "\n",
        "    # Treina e retorna histórico\n",
        "    history = train_and_validate(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        device=DEVICE,\n",
        "        epochs=EPOCHS,\n",
        "        save_path=f\"{MODEL_NAME}_run{i}.pth\",\n",
        "        num_classes=num_classes\n",
        "    )\n",
        "\n",
        "    histories[f\"run_{i}\"] = history\n",
        "\n",
        "    # Salva histórico em .pt\n",
        "    torch.save(history, f\"history_{MODEL_NAME}_run{i}.pt\")\n",
        "\n",
        "    # Salva histórico em CSV\n",
        "    df = pd.DataFrame(history)\n",
        "    csv_path = f\"{MODEL_NAME}_run{i}_history.csv\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"Histórico salvo em CSV em {csv_path}\")\n",
        "\n",
        "    # Plota e salva gráficos\n",
        "    plot_history(history, model_name=f\"{MODEL_NAME}_run{i}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dabbee06",
      "metadata": {
        "id": "dabbee06"
      },
      "source": [
        "## Treinamento\n",
        "\n",
        "Defina a função de custo e o otimizador do modelo. Em seguida, implemente o código de treinamento e treine-o. Ao final, exiba as curvas de treinamento e validação para a loss e a acurácia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d247d1dd",
      "metadata": {
        "id": "d247d1dd"
      },
      "outputs": [],
      "source": [
        "# Seu código aqui"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85323b96",
      "metadata": {
        "id": "85323b96"
      },
      "source": [
        "## Inferência\n",
        "\n",
        "Calcule algumas métricas como acurácia, matriz de confusão, etc. Em seguida, teste o modelo em novas imagens das classes correspondentes mas de outras fontes (outro buscador, fotos próprias, etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21c63e2a",
      "metadata": {
        "id": "21c63e2a"
      },
      "outputs": [],
      "source": [
        "# Seu código aqui"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}